{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now using GPU #2:\n",
      "Tesla V100-SXM2-32GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Namespace(dataset='cora', imb_ratio=10, net='GCN', n_layer=2, feat_dim=256, loss_type='bs', tam=False, reweight=False, pc_softmax=False, ens=False, renode=False, keep_prob=0.01, pred_temp=2, loss_name='re-weight', factor_focal=2.0, factor_cb=0.9999, rn_base=0.5, rn_max=1.5, tam_alpha=2.5, tam_beta=0.5, temp_phi=1.2, warmup=5)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Our code is based on GraphENS:\n",
    "https://github.com/JoonHyung-Park/GraphENS\n",
    "\"\"\"\n",
    "\n",
    "import os.path as osp\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nets import *\n",
    "from data_utils import *\n",
    "from args import parse_args\n",
    "from models import *\n",
    "from losses import *\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "import statistics\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import dataloader\n",
    "\n",
    "from utils import *\n",
    "\n",
    "# Set GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.argv = [\"\"]\n",
    "args = parse_args()\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.set_device(2)\n",
    "    device_id = torch.cuda.current_device()\n",
    "    print(f\"Now using GPU #{device_id}:\\n{torch.cuda.get_device_name(device_id)}\")\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30 µs, sys: 0 ns, total: 30 µs\n",
      "Wall time: 34.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "class GraphBalanceAugmenter():\n",
    "\n",
    "    MODE_SPACE = ['dummy', 'uncertainty', 'topology', 'discrepancy']\n",
    "    \n",
    "    def __init__(self, x, edge_index, y_train, train_mask, device, \n",
    "                 mode:str='uncertainty', risk_pow:int=None, risk_mul:float=None):\n",
    "        \n",
    "        # parameter check\n",
    "        assert mode in self.MODE_SPACE\n",
    "        assert risk_pow is None or risk_pow >=0\n",
    "        assert risk_mul is None or (risk_mul <= 1 and risk_mul >=0)\n",
    "        self.mode = mode\n",
    "        self.risk_pow = risk_pow\n",
    "        self.risk_mul = risk_mul\n",
    "\n",
    "        # initialization\n",
    "        x, edge_index, y_train, train_mask = \\\n",
    "            x.to(device), edge_index.to(device), y_train.to(device), train_mask.to(device)\n",
    "        self.n_node = x.shape[0]\n",
    "        self.n_edge = edge_index.shape[1]\n",
    "        self.classes = y_train.unique()\n",
    "        self.y_virtual = y_train.unique()\n",
    "        self.n_class = y_train.unique().shape[0]\n",
    "        self.y_train = y_train\n",
    "        self.train_mask = train_mask\n",
    "        self.adj = index_to_adj(x, edge_index)\n",
    "        self.class_num_list = self.y_train.bincount()\n",
    "        self.class_weights = self.class_num_list / self.class_num_list.max()\n",
    "        self.empty_edge_index = torch.zeros(2, 0, dtype=torch.long, device=device)\n",
    "        self.device = device\n",
    "\n",
    "    @staticmethod\n",
    "    def edge_sampling(edge_index, edge_sampling_proba):\n",
    "        assert edge_sampling_proba.min() >=0 and edge_sampling_proba.max() <= 1\n",
    "        edge_sample_mask = (torch.rand_like(edge_sampling_proba) < edge_sampling_proba)\n",
    "        return edge_index[:, edge_sample_mask]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_group_mean(values, labels, classes):\n",
    "        new_values = torch.zeros_like(values)\n",
    "        for i in classes:\n",
    "            mask = labels == i\n",
    "            new_values[mask] = values[mask].mean()\n",
    "        return new_values\n",
    "\n",
    "    @staticmethod\n",
    "    def get_virtual_node_features(x, y_pred, classes):\n",
    "        return torch.stack([x[y_pred == label].mean(axis=0) for label in classes])\n",
    "\n",
    "    @staticmethod\n",
    "    def get_connectivity_distribution(y_pred, adj, n_class, n_node):\n",
    "        # get connectivity label distribution\n",
    "        y_pred_mat = y_pred.mul(adj)\n",
    "        y_pred_mat[~adj.bool()] = n_class\n",
    "        y_neighbor_distr = torch.zeros(\n",
    "            n_class+1, n_node, dtype=torch.int, device=y_pred_mat.device\n",
    "        ).scatter_add_(0, y_pred_mat.T, torch.ones(\n",
    "            n_node, n_node, dtype=torch.int, device=y_pred_mat.device\n",
    "        ))[:n_cls].T.float()\n",
    "        # row-wise normalization\n",
    "        y_neighbor_distr /= y_neighbor_distr.sum(axis=1).reshape(-1, 1)\n",
    "        y_neighbor_distr = y_neighbor_distr.nan_to_num(0)\n",
    "        return y_neighbor_distr\n",
    "\n",
    "    def adapt_labels_and_train_mask(self, y:torch.Tensor, train_mask:torch.Tensor):\n",
    "        if self.mode == 'dummy':\n",
    "            return y, train_mask\n",
    "        new_y = torch.concat([y, self.y_virtual])\n",
    "        new_train_mask = torch.concat([train_mask, torch.ones_like(self.y_virtual).bool()])\n",
    "        return new_y, new_train_mask\n",
    "\n",
    "    def augment(self, model, x, edge_index):\n",
    "\n",
    "        train_mask = self.train_mask\n",
    "        # do nothing if mode is 'dummy'\n",
    "        if self.mode == 'dummy':\n",
    "            return (x, edge_index, {'time': 0, 'node_ratio': 1, 'edge_ratio': 1})\n",
    "\n",
    "        # initialization\n",
    "        start_time = time.time()\n",
    "        y_pred_proba = predict_proba_tensor(model, x, edge_index)\n",
    "        y_pred = y_pred_proba.argmax(axis=1)\n",
    "        y_pred[train_mask] = self.y_train\n",
    "        y_neighbor_distr = self.get_connectivity_distribution(y_pred, self.adj, self.n_class, self.n_node)\n",
    "\n",
    "        # compute node_risk and virtual link probability\n",
    "        node_risk = self.get_node_risk(y_pred_proba, y_pred)\n",
    "        node_similarities = self.get_node_similarity_to_candidate_classes(y_pred_proba, y_neighbor_distr)\n",
    "        virtual_link_proba = self.get_virual_link_proba(node_similarities, y_pred)\n",
    "        # assign link probability w.r.t node risk\n",
    "        virtual_link_proba *= node_risk.reshape(-1, 1)\n",
    "\n",
    "        # sample virtual edge_index w.r.t given probability\n",
    "        virtual_adj = virtual_link_proba.T.to_sparse().coalesce()\n",
    "        edge_index_candidates, edge_sampling_proba = virtual_adj.indices(), virtual_adj.values()\n",
    "        virtual_edge_index = self.edge_sampling(edge_index_candidates, edge_sampling_proba)\n",
    "        virtual_edge_index[0] += self.n_node    # adjust index to match original node index\n",
    "        virtual_edge_index = to_undirected(virtual_edge_index)\n",
    "\n",
    "        # compute virtual node features\n",
    "        x_virtual = self.get_virtual_node_features(x, y_pred, self.classes)\n",
    "        \n",
    "        # concatenate results\n",
    "        used_time = time.time() - start_time\n",
    "        x_aug = torch.concat([x, x_virtual])\n",
    "        edge_index_aug = torch.concat([edge_index, virtual_edge_index], axis=1)\n",
    "        info = {\n",
    "            'time': used_time,\n",
    "            'node_ratio': x_aug.shape[0] / x.shape[0],\n",
    "            'edge_ratio': edge_index_aug.shape[1] / edge_index.shape[1],\n",
    "        }\n",
    "        return x_aug, edge_index_aug, info\n",
    "    \n",
    "    def get_node_risk(self, y_pred_proba, y_pred):\n",
    "        # compute node uncertainty\n",
    "        node_uncertainty = 1 - y_pred_proba.max(axis=1).values\n",
    "        # compute class-aware relative uncertainty\n",
    "        node_unc_class_mean = self.get_group_mean(node_uncertainty, y_pred, self.classes)\n",
    "        node_risk = (node_uncertainty - node_unc_class_mean).clip(min=0)\n",
    "        # lower the risk of minority class nodes\n",
    "        node_risk *= self.class_weights[y_pred]\n",
    "        # rescale node risk by given hyper-parameter\n",
    "        # node_risk /= node_risk.max()\n",
    "        if self.risk_pow:\n",
    "            node_risk = node_risk.pow(self.risk_pow)\n",
    "        if self.risk_mul:\n",
    "            node_risk *= self.risk_mul\n",
    "        return node_risk\n",
    "\n",
    "    def get_node_similarity_to_candidate_classes(self, y_pred_proba, y_neighbor_distr):\n",
    "        mode = self.mode\n",
    "        if mode == 'uncertainty':\n",
    "            node_similarities = y_pred_proba\n",
    "        elif mode == 'topology':\n",
    "            node_similarities = y_neighbor_distr\n",
    "        elif mode == 'discrepancy':\n",
    "            node_similarities = y_neighbor_distr - y_pred_proba\n",
    "        else: raise NotImplementedError\n",
    "        return node_similarities\n",
    "        \n",
    "    def get_virual_link_proba(self, node_similarities, y_pred):\n",
    "        # set similarity to current predicted class as 0\n",
    "        node_similarities *= (1 - F.one_hot(y_pred, num_classes=self.n_class))\n",
    "        node_similarities = node_similarities.clip(min=0)\n",
    "        # row-wise normalize\n",
    "        node_similarities /= node_similarities.sum(axis=1).reshape(-1, 1)\n",
    "        virtual_link_proba = node_similarities.nan_to_num(0)\n",
    "        return virtual_link_proba\n",
    "\n",
    "\n",
    "# gba = GraphBalanceAugmenter(\n",
    "#     x=data.x, edge_index=data.edge_index, y_train=data.y[data_train_mask], \n",
    "#     train_mask=data_train_mask, device=device, \n",
    "#     mode='uncertainty', risk_pow=None, risk_mul=None,\n",
    "# )\n",
    "# x_aug, edge_index_aug, info = gba.augment(model, data.x, data.edge_index)\n",
    "# y_aug, train_mask_aug = gba.adapt_labels_and_train_mask(data.y, data_train_mask)\n",
    "# info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse, mask_to_index\n",
    "\n",
    "\n",
    "## For GraphENS ##\n",
    "def backward_hook(module, grad_input, grad_output):\n",
    "    global saliency\n",
    "    saliency = grad_input[0].data\n",
    "\n",
    "\n",
    "def tensor_hook(grad):\n",
    "    global saliency\n",
    "    saliency = grad.data\n",
    "\n",
    "\n",
    "def train():\n",
    "    global data, class_weight, gba, runtime_info\n",
    "    global class_num_list, idx_info, prev_out, aggregator, renode_loss, tail_classes\n",
    "    global data_train_mask, data_val_mask, data_test_mask\n",
    "    global model, optimizer, criterion, scheduler, epoch, neighbor_dist_list\n",
    "\n",
    "    if args.gba:\n",
    "        x_aug, edge_index_aug, info = gba.augment(model, data.x, data.edge_index)\n",
    "        y_aug, train_mask_aug = gba.adapt_labels_and_train_mask(data.y, data_train_mask)\n",
    "        runtime_info.append([info[\"time\"], info[\"node_ratio\"], info[\"edge_ratio\"]])\n",
    "        if args.debug and epoch % args.num_epochs == 0:\n",
    "            print(\n",
    "                \"Epoch: {:d} | aug_time {} ms | node_ratio {:.3%} | edge_ratio {:.3%}\".format(\n",
    "                    epoch, info[\"time\"] * 1000, info[\"node_ratio\"], info[\"edge_ratio\"]\n",
    "                )\n",
    "            )\n",
    "    else:\n",
    "        x_aug, y_aug = data.x.clone(), data.y.clone()\n",
    "        edge_index_aug = data.edge_index.clone()\n",
    "        train_mask_aug = data_train_mask.clone()\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if args.ens:\n",
    "        # Hook saliency map of input features\n",
    "        model.conv1.temp_weight.register_backward_hook(backward_hook)\n",
    "        # Sampling source and destination nodes\n",
    "        sampling_src_idx, sampling_dst_idx = sampling_idx_individual_dst(\n",
    "            class_num_list, idx_info, device\n",
    "        )\n",
    "        beta = torch.distributions.beta.Beta(2, 2)\n",
    "        lam = beta.sample((len(sampling_src_idx),)).unsqueeze(1)\n",
    "        ori_saliency = saliency[: x_aug.shape[0]] if (saliency != None) else None\n",
    "        # Augment nodes\n",
    "        if epoch > args.warmup:\n",
    "            with torch.no_grad():\n",
    "                prev_out = aggregator(prev_out, edge_index_aug)\n",
    "                prev_out = F.softmax(prev_out / args.pred_temp, dim=1).detach().clone()\n",
    "            new_edge_index, dist_kl = neighbor_sampling(\n",
    "                x_aug.size(0),\n",
    "                edge_index_aug,\n",
    "                sampling_src_idx,\n",
    "                sampling_dst_idx,\n",
    "                neighbor_dist_list,\n",
    "                prev_out,\n",
    "            )\n",
    "            new_x = saliency_mixup(\n",
    "                x_aug,\n",
    "                sampling_src_idx,\n",
    "                sampling_dst_idx,\n",
    "                lam,\n",
    "                ori_saliency,\n",
    "                dist_kl=dist_kl,\n",
    "                keep_prob=args.keep_prob,\n",
    "            )\n",
    "        else:\n",
    "            new_edge_index = duplicate_neighbor(\n",
    "                x_aug.size(0), data.edge_index, sampling_src_idx\n",
    "            )\n",
    "            dist_kl, ori_saliency = None, None\n",
    "            new_x = saliency_mixup(\n",
    "                x_aug,\n",
    "                sampling_src_idx,\n",
    "                sampling_dst_idx,\n",
    "                lam,\n",
    "                ori_saliency,\n",
    "                dist_kl=dist_kl,\n",
    "            )\n",
    "        new_x.requires_grad = True\n",
    "        # Get predictions\n",
    "        output = model(new_x, new_edge_index)\n",
    "        prev_out = (output[: x_aug.size(0)]).detach().clone()  # logit propagation\n",
    "        ## Train_mask modification ##\n",
    "        add_num = output.shape[0] - train_mask_aug.shape[0]\n",
    "        new_train_mask = torch.ones(add_num, dtype=torch.bool, device=x_aug.device)\n",
    "        new_train_mask = torch.cat((train_mask_aug, new_train_mask), dim=0)\n",
    "        ## Label modification ##\n",
    "        _new_y = y_aug[sampling_src_idx].clone()\n",
    "        new_y = torch.cat((y_aug[train_mask_aug], _new_y), dim=0)\n",
    "        ## Compute Loss ##\n",
    "        loss = criterion(output[new_train_mask], new_y)\n",
    "        if args.debug and epoch % args.num_epochs == 0:\n",
    "            print(\n",
    "                f\"GraphENS node_ratio {new_x.shape[0]/x_aug.shape[0]:.2%} edge_ratio {new_edge_index.shape[1]/edge_index_aug.shape[1]:.2%}\"\n",
    "            )\n",
    "\n",
    "    elif args.renode:\n",
    "        ## ReNode ##\n",
    "        if args.gba and data.rn_weight.shape[0] < train_mask_aug.shape[0]:\n",
    "            data.rn_weight = torch.cat(\n",
    "                [data.rn_weight, torch.ones(n_cls, device=device)]\n",
    "            )\n",
    "        output = model(x_aug, edge_index_aug)\n",
    "        sup_logits = output[train_mask_aug]\n",
    "        cls_loss = renode_loss.compute(sup_logits, y_aug[train_mask_aug].to(device))\n",
    "        loss = torch.sum(\n",
    "            cls_loss * data.rn_weight[train_mask_aug].to(device)\n",
    "        ) / cls_loss.size(0)\n",
    "\n",
    "    elif args.graphsmote:\n",
    "        ## GraphSMOTE ##\n",
    "        num_nodes_aug = x_aug.shape[0]\n",
    "        embed = model.get_embed(x_aug, edge_index_aug)\n",
    "        idx_train = mask_to_index(train_mask_aug)\n",
    "        adj = to_dense_adj(edge_index_aug, max_num_nodes=num_nodes_aug)[0]\n",
    "        embed_new, labels_new, idx_train_new, adj_up = recon_upsample(\n",
    "            embed,\n",
    "            y_aug,\n",
    "            idx_train,\n",
    "            adj=adj.detach(),\n",
    "            portion=0,\n",
    "            tail_classes=tail_classes,\n",
    "        )\n",
    "        generated_G = gs_decoder(embed_new)\n",
    "        loss_rec = graphsmote.adj_mse_loss(\n",
    "            generated_G[:num_nodes_aug, :][:, :num_nodes_aug], adj.detach()\n",
    "        )\n",
    "        adj_new = copy.deepcopy(generated_G.detach())\n",
    "        threshold = 0.5\n",
    "        adj_new[adj_new < threshold] = 0.0\n",
    "        adj_new[adj_new >= threshold] = 1.0\n",
    "        adj_new = torch.mul(adj_up.to(adj_new.device), adj_new)\n",
    "        adj_new[:num_nodes_aug, :][:, :num_nodes_aug] = adj.detach()\n",
    "        adj_new, _ = dense_to_sparse(adj_new)\n",
    "        output = model.embed_to_pred(embed_new, adj_new)\n",
    "        loss = (\n",
    "            criterion(output[idx_train_new], labels_new[idx_train_new])\n",
    "            + loss_rec * 1e-6\n",
    "        )\n",
    "        if args.debug and epoch % args.num_epochs == 0:\n",
    "            print(\n",
    "                f\"GraphSMOTE node_ratio {embed_new.shape[0]/x_aug.shape[0]:.2%} edge_ratio {adj_new.shape[1]/edge_index_aug.shape[1]:.2%}\"\n",
    "            )\n",
    "\n",
    "    elif args.smote:\n",
    "        ## SMOTE ##\n",
    "        adj, features, labels = edge_index_aug, x_aug, y_aug\n",
    "        idx_train = mask_to_index(train_mask_aug)\n",
    "        adj_new, features_new, labels_new, idx_train_new = src_smote(\n",
    "            adj, features, labels, idx_train, portion=0, tail_classes=tail_classes\n",
    "        )\n",
    "        output = model(features_new, adj_new.indices())\n",
    "        loss = criterion(output[idx_train_new], labels_new[idx_train_new])\n",
    "        if args.debug and epoch % args.num_epochs == 0:\n",
    "            print(\n",
    "                f\"SMOTE node_ratio {features_new.shape[0]/x_aug.shape[0]:.2%} edge_ratio {adj_new.indices().shape[1]/edge_index_aug.shape[1]:.2%}\"\n",
    "            )\n",
    "\n",
    "    elif args.resample:\n",
    "        ## Resampling ##\n",
    "        adj, features, labels = edge_index_aug, x_aug, y_aug\n",
    "        idx_train = mask_to_index(train_mask_aug)\n",
    "        adj_new, features_new, labels_new, idx_train_new = src_upsample(\n",
    "            adj, features, labels, idx_train, portion=0, tail_classes=tail_classes\n",
    "        )\n",
    "        output = model(features_new, adj_new.indices())\n",
    "        loss = criterion(output[idx_train_new], labels_new[idx_train_new])\n",
    "        if args.debug and epoch % args.num_epochs == 0:\n",
    "            print(\n",
    "                f\"Oversample node_ratio {features_new.shape[0]/x_aug.shape[0]:.2%} edge_ratio {adj_new.indices().shape[1]/edge_index_aug.shape[1]:.2%}\"\n",
    "            )\n",
    "\n",
    "    elif args.reweight:\n",
    "        ## Reweight ##\n",
    "        output = model(x_aug, edge_index_aug)\n",
    "        loss = criterion(\n",
    "            output[train_mask_aug], y_aug[train_mask_aug], weight=class_weight\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        ## Vanilla ##\n",
    "        output = model(x_aug, edge_index_aug)\n",
    "        loss = criterion(output[train_mask_aug], y_aug[train_mask_aug])\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    if args.graphsmote:\n",
    "        gs_decoder_optimizer.zero_grad()\n",
    "        gs_decoder_optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        output = model(data.x, data.edge_index)\n",
    "        val_loss = F.cross_entropy(output[data_val_mask], data.y[data_val_mask])\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits = model(data.x, data.edge_index)\n",
    "    accs, baccs, f1s = [], [], []\n",
    "\n",
    "    if args.pc_softmax:\n",
    "        logits = pc_softmax(logits, class_num_list)\n",
    "\n",
    "    for i, mask in enumerate([data_train_mask, data_val_mask, data_test_mask]):\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        y_pred = pred.cpu().numpy()\n",
    "        y_true = data.y[mask].cpu().numpy()\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "        accs.append(acc)\n",
    "        baccs.append(bacc)\n",
    "        f1s.append(f1)\n",
    "\n",
    "    return accs, baccs, f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semi_train_val_test_split(data, n_train_num_per_class=20, test_ratio=0.5):\n",
    "    from torch_geometric.utils import mask_to_index, index_to_mask\n",
    "\n",
    "    n_nodes = data.y.shape[0]\n",
    "    train_index, val_index, test_index = [], [], []\n",
    "    for label in data.y.unique():\n",
    "        cls_index = mask_to_index(data.y == label)\n",
    "        n_cls_nodes = len(cls_index)\n",
    "        n_train_nodes = n_train_num_per_class\n",
    "        n_val_nodes = int((n_cls_nodes - n_train_nodes) * (1 - test_ratio))\n",
    "        n_test_nodes = int((n_cls_nodes - n_train_nodes) * test_ratio)\n",
    "        perm_index = torch.randperm(len(cls_index))\n",
    "        cls_train_index = cls_index[perm_index[:n_train_nodes]]\n",
    "        cls_val_index = cls_index[\n",
    "            perm_index[n_train_nodes : n_train_nodes + n_val_nodes]\n",
    "        ]\n",
    "        cls_test_index = cls_index[perm_index[n_train_nodes + n_val_nodes :]]\n",
    "        train_index.append(cls_train_index)\n",
    "        val_index.append(cls_val_index)\n",
    "        test_index.append(cls_test_index)\n",
    "\n",
    "    device = data.y.device\n",
    "    train_mask = index_to_mask(torch.concat(train_index), size=n_nodes).to(device)\n",
    "    val_mask = index_to_mask(torch.concat(val_index), size=n_nodes).to(device)\n",
    "    test_mask = index_to_mask(torch.concat(test_index), size=n_nodes).to(device)\n",
    "    return train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to ./results/IR(10)-rep(2)-epoch(100)-gnn['GCN']-data['Cora']-bsl['vanilla', 'reweight', 'resample', 'renode', 'smote', 'graphsmote', 'graphens'].csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 48.57it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 67.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: vanilla, gba: dummy\n",
      "node_ratio: 0.000% | edge_ratio: 0.000% | Runtime: 0.0000 s\n",
      "Val Acc F1: 0.6380, Test Disp 0.2797 +- 0.0139 Acc: 0.6830 +- 0.0060, BAcc: 0.6228 +- 0.0113, F1: 0.6160 +- 0.0079\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 48.31it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 51.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: vanilla, gba: uncertainty\n",
      "node_ratio: 0.258% | edge_ratio: 3.539% | Runtime: 0.0050 s\n",
      "Val Acc F1: 0.6288, Test Disp 0.2421 +- 0.0309 Acc: 0.6755 +- 0.0135, BAcc: 0.6282 +- 0.0073, F1: 0.6103 +- 0.0170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 48.98it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 49.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: vanilla, gba: topology\n",
      "node_ratio: 0.258% | edge_ratio: 1.534% | Runtime: 0.0051 s\n",
      "Val Acc F1: 0.6725, Test Disp 0.2365 +- 0.0119 Acc: 0.7180 +- 0.0190, BAcc: 0.6708 +- 0.0214, F1: 0.6584 +- 0.0259\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 65.51it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 67.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: reweight, gba: dummy\n",
      "node_ratio: 0.000% | edge_ratio: 0.000% | Runtime: 0.0000 s\n",
      "Val Acc F1: 0.6799, Test Disp 0.2418 +- 0.0002 Acc: 0.7270 +- 0.0140, BAcc: 0.6720 +- 0.0054, F1: 0.6790 +- 0.0052\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 50.57it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 50.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: reweight, gba: uncertainty\n",
      "node_ratio: 0.258% | edge_ratio: 1.900% | Runtime: 0.0049 s\n",
      "Val Acc F1: 0.6934, Test Disp 0.2046 +- 0.0024 Acc: 0.7200 +- 0.0190, BAcc: 0.6873 +- 0.0001, F1: 0.6727 +- 0.0111\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 50.70it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 49.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: reweight, gba: topology\n",
      "node_ratio: 0.258% | edge_ratio: 1.115% | Runtime: 0.0049 s\n",
      "Val Acc F1: 0.7162, Test Disp 0.2189 +- 0.0021 Acc: 0.7205 +- 0.0015, BAcc: 0.7112 +- 0.0017, F1: 0.6842 +- 0.0013\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 55.34it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: resample, gba: dummy\n",
      "node_ratio: 0.000% | edge_ratio: 0.000% | Runtime: 0.0000 s\n",
      "Val Acc F1: 0.6156, Test Disp 0.2944 +- 0.0276 Acc: 0.6700 +- 0.0160, BAcc: 0.6077 +- 0.0210, F1: 0.5985 +- 0.0352\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 45.21it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 44.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: resample, gba: uncertainty\n",
      "node_ratio: 0.258% | edge_ratio: 2.383% | Runtime: 0.0047 s\n",
      "Val Acc F1: 0.7327, Test Disp 0.1590 +- 0.0490 Acc: 0.7580 +- 0.0100, BAcc: 0.7360 +- 0.0124, F1: 0.7208 +- 0.0071\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 44.98it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 44.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: resample, gba: topology\n",
      "node_ratio: 0.258% | edge_ratio: 1.190% | Runtime: 0.0048 s\n",
      "Val Acc F1: 0.7247, Test Disp 0.2174 +- 0.0018 Acc: 0.7755 +- 0.0025, BAcc: 0.7262 +- 0.0108, F1: 0.7250 +- 0.0038\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 65.71it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 64.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: renode, gba: dummy\n",
      "node_ratio: 0.000% | edge_ratio: 0.000% | Runtime: 0.0000 s\n",
      "Val Acc F1: 0.6474, Test Disp 0.2633 +- 0.0087 Acc: 0.7020 +- 0.0160, BAcc: 0.6421 +- 0.0291, F1: 0.6435 +- 0.0254\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 48.82it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 50.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: renode, gba: uncertainty\n",
      "node_ratio: 0.258% | edge_ratio: 1.942% | Runtime: 0.0049 s\n",
      "Val Acc F1: 0.7043, Test Disp 0.1832 +- 0.0316 Acc: 0.7085 +- 0.0155, BAcc: 0.7023 +- 0.0125, F1: 0.6827 +- 0.0151\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 50.44it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 50.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: renode, gba: topology\n",
      "node_ratio: 0.258% | edge_ratio: 1.090% | Runtime: 0.0049 s\n",
      "Val Acc F1: 0.7225, Test Disp 0.1886 +- 0.0310 Acc: 0.7350 +- 0.0160, BAcc: 0.7065 +- 0.0183, F1: 0.6997 +- 0.0226\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 46.42it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 45.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: smote, gba: dummy\n",
      "node_ratio: 0.000% | edge_ratio: 0.000% | Runtime: 0.0000 s\n",
      "Val Acc F1: 0.6106, Test Disp 0.3037 +- 0.0025 Acc: 0.6720 +- 0.0030, BAcc: 0.6024 +- 0.0030, F1: 0.5925 +- 0.0032\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 40.20it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 40.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: smote, gba: uncertainty\n",
      "node_ratio: 0.258% | edge_ratio: 2.425% | Runtime: 0.0047 s\n",
      "Val Acc F1: 0.7325, Test Disp 0.1882 +- 0.0041 Acc: 0.7545 +- 0.0055, BAcc: 0.7261 +- 0.0047, F1: 0.7130 +- 0.0021\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 40.16it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 40.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: smote, gba: topology\n",
      "node_ratio: 0.258% | edge_ratio: 1.228% | Runtime: 0.0046 s\n",
      "Val Acc F1: 0.7356, Test Disp 0.2175 +- 0.0022 Acc: 0.7670 +- 0.0030, BAcc: 0.7282 +- 0.0003, F1: 0.7140 +- 0.0042\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.34it/s]\n",
      "100%|██████████| 100/100 [00:05<00:00, 18.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: graphsmote, gba: dummy\n",
      "node_ratio: 0.000% | edge_ratio: 0.000% | Runtime: 0.0000 s\n",
      "Val Acc F1: 0.6868, Test Disp 0.2151 +- 0.0024 Acc: 0.7450 +- 0.0160, BAcc: 0.6908 +- 0.0050, F1: 0.6942 +- 0.0127\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 22.36it/s]\n",
      "100%|██████████| 100/100 [00:04<00:00, 22.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: graphsmote, gba: uncertainty\n",
      "node_ratio: 0.258% | edge_ratio: 2.287% | Runtime: 0.0050 s\n",
      "Val Acc F1: 0.6958, Test Disp 0.2167 +- 0.0058 Acc: 0.7290 +- 0.0160, BAcc: 0.6916 +- 0.0065, F1: 0.6820 +- 0.0117\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 22.50it/s]\n",
      "100%|██████████| 100/100 [00:04<00:00, 22.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: graphsmote, gba: topology\n",
      "node_ratio: 0.258% | edge_ratio: 1.251% | Runtime: 0.0049 s\n",
      "Val Acc F1: 0.7250, Test Disp 0.2188 +- 0.0045 Acc: 0.7470 +- 0.0100, BAcc: 0.7069 +- 0.0045, F1: 0.6966 +- 0.0063\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.54it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 52.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: graphens, gba: dummy\n",
      "node_ratio: 0.000% | edge_ratio: 0.000% | Runtime: 0.0000 s\n",
      "Val Acc F1: 0.7257, Test Disp 0.2170 +- 0.0046 Acc: 0.7410 +- 0.0030, BAcc: 0.7017 +- 0.0102, F1: 0.6936 +- 0.0066\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 41.22it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 40.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: graphens, gba: uncertainty\n",
      "node_ratio: 0.258% | edge_ratio: 2.422% | Runtime: 0.0055 s\n",
      "Val Acc F1: 0.7290, Test Disp 0.2209 +- 0.0029 Acc: 0.7730 +- 0.0100, BAcc: 0.7199 +- 0.0080, F1: 0.7164 +- 0.0121\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 41.04it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 41.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 2, Dataset: Cora, ratio: 10, net: GCN, n_layer: 3, feat_dim: 256, baseline: graphens, gba: topology\n",
      "node_ratio: 0.258% | edge_ratio: 1.364% | Runtime: 0.0055 s\n",
      "Val Acc F1: 0.7335, Test Disp 0.2195 +- 0.0020 Acc: 0.7765 +- 0.0115, BAcc: 0.7260 +- 0.0075, F1: 0.7219 +- 0.0124\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_space = [\"Cora\"]\n",
    "# dataset_space = ['Cora', 'CiteSeer', 'PubMed', 'CS', 'Physics']\n",
    "\n",
    "# model_space = ['GCN', 'GAT', 'SAGE']\n",
    "model_space = [\"GCN\"]\n",
    "baseline_space = [\n",
    "    \"vanilla\",\n",
    "    \"reweight\",\n",
    "    \"resample\",\n",
    "    \"renode\",\n",
    "    \"smote\",\n",
    "    \"graphsmote\",\n",
    "    \"graphens\",\n",
    "]\n",
    "# baseline_space = ['graphens', 'graphsmote', 'renode', 'smote', 'resample', 'reweight', 'vanilla']\n",
    "# baseline_space = ['vanilla']\n",
    "\n",
    "setting_space = [\"dummy\", \"uncertainty\", \"topology\"]\n",
    "# setting_space = [\"uncertainty\", \"topology\"]\n",
    "# setting_space = ['uncertainty']\n",
    "save_path = \"./results\"\n",
    "\n",
    "sys.argv = [\"\"]\n",
    "args = parse_args()\n",
    "\n",
    "args.disable_tqdm = False\n",
    "args.debug = False\n",
    "# args.debug = True\n",
    "\n",
    "args.imb_ratio = 10\n",
    "# args.imb_ratio = 10\n",
    "args.loss_type = \"ce\"\n",
    "args.tam = False\n",
    "args.pc_softmax = False\n",
    "args.net = \"GCN\"\n",
    "args.feat_dim = 256\n",
    "args.n_layer = 3\n",
    "args.num_epochs = 100\n",
    "args.repeatition = 2\n",
    "args.save_results = True\n",
    "# args.save_results = False\n",
    "\n",
    "if args.save_results:\n",
    "    file_name = f\"IR({args.imb_ratio})-rep({args.repeatition})-epoch({args.num_epochs})-gnn{str(model_space)}-data{str(dataset_space)}-bsl{str(baseline_space)}.csv\"\n",
    "    args.res_path = f\"{save_path}/{file_name}\"\n",
    "    print(f\"Saving to {args.res_path} ...\")\n",
    "\n",
    "args.gba = False\n",
    "args.ens = False\n",
    "args.reweight = False\n",
    "args.renode = False\n",
    "args.graphsmote = False\n",
    "args.smote = False\n",
    "args.resample = False\n",
    "\n",
    "# args.ens = True\n",
    "# args.reweight = True\n",
    "args.gba = {\n",
    "    \"risk_pow\": None,\n",
    "    # 'risk_pow': 2,\n",
    "    \"risk_mul\": None,\n",
    "    # 'risk_mul': .5,\n",
    "    # 'mode': 'dummy',\n",
    "    # 'mode': 'uncertainty',\n",
    "    # 'mode': 'topology',\n",
    "    # 'mode': 'discrepancy',\n",
    "}\n",
    "\n",
    "all_results = []\n",
    "\n",
    "\n",
    "for dataset_name in dataset_space:\n",
    "    args.dataset = dataset_name\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for baseline_name in baseline_space:\n",
    "        if baseline_name == \"vanilla\":\n",
    "            (\n",
    "                args.reweight,\n",
    "                args.renode,\n",
    "                args.resample,\n",
    "                args.smote,\n",
    "                args.graphsmote,\n",
    "                args.ens,\n",
    "            ) = (False, False, False, False, False, False)\n",
    "        elif baseline_name == \"reweight\":\n",
    "            (\n",
    "                args.reweight,\n",
    "                args.renode,\n",
    "                args.resample,\n",
    "                args.smote,\n",
    "                args.graphsmote,\n",
    "                args.ens,\n",
    "            ) = (True, False, False, False, False, False)\n",
    "        elif baseline_name == \"renode\":\n",
    "            (\n",
    "                args.reweight,\n",
    "                args.renode,\n",
    "                args.resample,\n",
    "                args.smote,\n",
    "                args.graphsmote,\n",
    "                args.ens,\n",
    "            ) = (False, True, False, False, False, False)\n",
    "        elif baseline_name == \"resample\":\n",
    "            (\n",
    "                args.reweight,\n",
    "                args.renode,\n",
    "                args.resample,\n",
    "                args.smote,\n",
    "                args.graphsmote,\n",
    "                args.ens,\n",
    "            ) = (False, False, True, False, False, False)\n",
    "        elif baseline_name == \"smote\":\n",
    "            (\n",
    "                args.reweight,\n",
    "                args.renode,\n",
    "                args.resample,\n",
    "                args.smote,\n",
    "                args.graphsmote,\n",
    "                args.ens,\n",
    "            ) = (False, False, False, True, False, False)\n",
    "        elif baseline_name == \"graphsmote\":\n",
    "            (\n",
    "                args.reweight,\n",
    "                args.renode,\n",
    "                args.resample,\n",
    "                args.smote,\n",
    "                args.graphsmote,\n",
    "                args.ens,\n",
    "            ) = (False, False, False, False, True, False)\n",
    "        elif baseline_name == \"graphens\":\n",
    "            (\n",
    "                args.reweight,\n",
    "                args.renode,\n",
    "                args.resample,\n",
    "                args.smote,\n",
    "                args.graphsmote,\n",
    "                args.ens,\n",
    "            ) = (False, False, False, False, False, True)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "\n",
    "        for model_name in model_space:\n",
    "            args.net = model_name\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            for gba_setting in setting_space:\n",
    "                args.gba[\"mode\"] = gba_setting\n",
    "\n",
    "                runtime_info = []\n",
    "\n",
    "                ## Log for Experiment Setting ##\n",
    "                setting_log = \"Runs: {}, Dataset: {}, ratio: {}, net: {}, n_layer: {}, feat_dim: {}, baseline: {}, gba: {}\".format(\n",
    "                    args.repeatition,\n",
    "                    args.dataset,\n",
    "                    str(args.imb_ratio),\n",
    "                    args.net,\n",
    "                    str(args.n_layer),\n",
    "                    str(args.feat_dim),\n",
    "                    baseline_name,\n",
    "                    gba_setting,\n",
    "                )\n",
    "\n",
    "                dataset = args.dataset\n",
    "                path = osp.join(\"data\", dataset)\n",
    "                dataset = get_dataset(dataset, path, split_type=\"public\")\n",
    "                data = dataset[0]\n",
    "                n_cls = data.y.max().item() + 1\n",
    "                data = data.to(device)\n",
    "                # print (f'Loaded dataset: {args.dataset} - ImbRatio {args.imb_ratio}')\n",
    "                # print (args)\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                repeatition = args.repeatition\n",
    "                seed = 100\n",
    "                (\n",
    "                    avg_val_acc_f1,\n",
    "                    avg_test_acc,\n",
    "                    avg_test_bacc,\n",
    "                    avg_test_f1,\n",
    "                    avg_test_disparity,\n",
    "                ) = ([], [], [], [], [])\n",
    "                avg_run_time, avg_node_ratio, avg_edge_ratio = [], [], []\n",
    "                for r in range(repeatition):\n",
    "\n",
    "                    runtime_info = []\n",
    "\n",
    "                    ## Fix seed ##\n",
    "                    torch.cuda.empty_cache()\n",
    "                    seed += 1\n",
    "                    torch.manual_seed(seed)\n",
    "                    torch.cuda.manual_seed(seed)\n",
    "                    torch.backends.cudnn.deterministic = True\n",
    "                    torch.backends.cudnn.benchmark = False\n",
    "                    random.seed(seed)\n",
    "                    np.random.seed(seed)\n",
    "\n",
    "                    if args.dataset in [\"Cora\", \"CiteSeer\", \"PubMed\"]:\n",
    "                        data_train_mask, data_val_mask, data_test_mask = (\n",
    "                            data.train_mask.clone(),\n",
    "                            data.val_mask.clone(),\n",
    "                            data.test_mask.clone(),\n",
    "                        )\n",
    "                    elif args.dataset in [\"CS\", \"Physics\"]:\n",
    "                        data_train_mask, data_val_mask, data_test_mask = (\n",
    "                            get_semi_train_val_test_split(data, 20)\n",
    "                        )\n",
    "                    else:\n",
    "                        data_train_mask, data_val_mask, data_test_mask = (\n",
    "                            get_graphens_train_val_test_split(data, 20)\n",
    "                        )\n",
    "                        # data_train_mask, data_val_mask, data_test_mask = get_train_val_test_split(data, 20)\n",
    "                        # data_train_mask, data_val_mask, data_test_mask = get_iid_train_val_test_split(data, train_ratio=0.6, test_ratio=0.2)\n",
    "\n",
    "                    ## Data statistic ##\n",
    "                    stats = data.y[data_train_mask]\n",
    "                    n_data = []\n",
    "                    for i in range(n_cls):\n",
    "                        data_num = (stats == i).sum()\n",
    "                        n_data.append(int(data_num.item()))\n",
    "                    idx_info = get_idx_info(data.y, n_cls, data_train_mask)\n",
    "                    class_num_list = n_data\n",
    "\n",
    "                    # for artificial imbalanced setting: only the last imb_class_num classes are imbalanced\n",
    "                    imb_class_num = n_cls // 2\n",
    "                    new_class_num_list = []\n",
    "                    tail_classes = []\n",
    "                    max_num = np.max(class_num_list[: n_cls - imb_class_num])\n",
    "                    for i in range(n_cls):\n",
    "                        if (\n",
    "                            args.imb_ratio > 1 and i > n_cls - 1 - imb_class_num\n",
    "                        ):  # only imbalance the last classes\n",
    "                            tail_classes.append(i)\n",
    "                            new_class_num_list.append(\n",
    "                                min(\n",
    "                                    int(max_num * (1.0 / args.imb_ratio)),\n",
    "                                    class_num_list[i],\n",
    "                                )\n",
    "                            )\n",
    "                        else:\n",
    "                            new_class_num_list.append(class_num_list[i])\n",
    "                    class_num_list = new_class_num_list\n",
    "\n",
    "                    if args.imb_ratio > 1:\n",
    "                        data_train_mask, idx_info = split_semi_dataset(\n",
    "                            len(data.x),\n",
    "                            n_data,\n",
    "                            n_cls,\n",
    "                            class_num_list,\n",
    "                            idx_info,\n",
    "                            data.x.device,\n",
    "                        )\n",
    "\n",
    "                    if args.ens:\n",
    "                        neighbor_dist_list = get_ins_neighbor_dist(\n",
    "                            data.y.size(0), data.edge_index, data_train_mask, device\n",
    "                        )\n",
    "                    else:\n",
    "                        neighbor_dist_list = None\n",
    "\n",
    "                    if args.ens:  # for getting saliency\n",
    "                        from ens_nets import *\n",
    "                    else:\n",
    "                        from nets import *\n",
    "\n",
    "                    if args.renode:\n",
    "                        ## ReNode method ##\n",
    "                        ## hyperparam ##\n",
    "                        pagerank_prob = 0.85\n",
    "\n",
    "                        # calculating the Personalized PageRank Matrix\n",
    "                        pr_prob = 1 - pagerank_prob\n",
    "                        A = index2dense(data.edge_index, data.num_nodes)\n",
    "                        A_hat = A.to(device) + torch.eye(A.size(0)).to(\n",
    "                            device\n",
    "                        )  # add self-loop\n",
    "                        D = torch.diag(torch.sum(A_hat, 1))\n",
    "                        D = D.inverse().sqrt()\n",
    "                        A_hat = torch.mm(torch.mm(D, A_hat), D)\n",
    "                        data.Pi = pr_prob * (\n",
    "                            (\n",
    "                                torch.eye(A.size(0)).to(device) - (1 - pr_prob) * A_hat\n",
    "                            ).inverse()\n",
    "                        )\n",
    "                        data.Pi = data.Pi.cpu()\n",
    "\n",
    "                        # calculating the ReNode Weight\n",
    "                        gpr_matrix = []  # the class-level influence distribution\n",
    "                        data.num_classes = n_cls\n",
    "                        for iter_c in range(data.num_classes):\n",
    "                            # iter_Pi = data.Pi[torch.tensor(target_data.train_node[iter_c]).long()]\n",
    "                            iter_Pi = data.Pi[\n",
    "                                idx_info[iter_c].long()\n",
    "                            ]  # check! is it same with above line?\n",
    "                            iter_gpr = torch.mean(iter_Pi, dim=0).squeeze()\n",
    "                            gpr_matrix.append(iter_gpr)\n",
    "\n",
    "                        temp_gpr = torch.stack(gpr_matrix, dim=0)\n",
    "                        temp_gpr = temp_gpr.transpose(0, 1)\n",
    "                        data.gpr = temp_gpr\n",
    "                        data.rn_weight = get_renode_weight(\n",
    "                            data, data_train_mask, args.rn_base, args.rn_max\n",
    "                        )  # ReNode Weight\n",
    "                        renode_loss = IMB_LOSS(\n",
    "                            args.loss_name,\n",
    "                            data,\n",
    "                            idx_info,\n",
    "                            args.factor_focal,\n",
    "                            args.factor_cb,\n",
    "                        )\n",
    "\n",
    "                    if args.graphsmote:\n",
    "                        gs_decoder = GSMOTEDecoder(\n",
    "                            nembed=args.feat_dim, dropout=0.1\n",
    "                        ).to(device)\n",
    "                        gs_decoder_optimizer = torch.optim.Adam(\n",
    "                            gs_decoder.parameters(), lr=0.001, weight_decay=5e-4\n",
    "                        )\n",
    "\n",
    "                    if args.gba:\n",
    "                        gba = GraphBalanceAugmenter(\n",
    "                            x=data.x,\n",
    "                            edge_index=data.edge_index,\n",
    "                            y_train=data.y[data_train_mask],\n",
    "                            train_mask=data_train_mask,\n",
    "                            device=device,\n",
    "                            mode=args.gba[\"mode\"],\n",
    "                            risk_pow=args.gba[\"risk_pow\"],\n",
    "                            risk_mul=args.gba[\"risk_mul\"],\n",
    "                        )\n",
    "\n",
    "                    ## Re-weight method ##\n",
    "                    class_weight = get_weight(args.reweight, class_num_list).to(device)\n",
    "\n",
    "                    ## Model Selection ##\n",
    "                    if args.net == \"GCN\":\n",
    "                        model = create_gcn(\n",
    "                            nfeat=dataset.num_features,\n",
    "                            nhid=args.feat_dim,\n",
    "                            nclass=n_cls,\n",
    "                            dropout=0.5,\n",
    "                            nlayer=args.n_layer,\n",
    "                        )\n",
    "                    elif args.net == \"GAT\":\n",
    "                        model = create_gat(\n",
    "                            nfeat=dataset.num_features,\n",
    "                            nhid=args.feat_dim,\n",
    "                            nclass=n_cls,\n",
    "                            dropout=0.5,\n",
    "                            nlayer=args.n_layer,\n",
    "                        )\n",
    "                    elif args.net == \"SAGE\":\n",
    "                        model = create_sage(\n",
    "                            nfeat=dataset.num_features,\n",
    "                            nhid=args.feat_dim,\n",
    "                            nclass=n_cls,\n",
    "                            dropout=0.5,\n",
    "                            nlayer=args.n_layer,\n",
    "                        )\n",
    "                    else:\n",
    "                        raise NotImplementedError(\"Not Implemented Architecture!\")\n",
    "\n",
    "                    ## Criterion Selection ##\n",
    "                    if args.loss_type == \"ce\":  # CE\n",
    "                        criterion = CrossEntropy()\n",
    "                    elif args.loss_type == \"bs\":\n",
    "                        criterion = BalancedSoftmax(class_num_list)\n",
    "                    else:\n",
    "                        raise NotImplementedError(\"Not Implemented Loss!\")\n",
    "\n",
    "                    model = model.to(device)\n",
    "                    criterion = criterion.to(device)\n",
    "\n",
    "                    # Set optimizer\n",
    "                    optimizer = torch.optim.Adam(\n",
    "                        [\n",
    "                            dict(params=model.reg_params, weight_decay=5e-4),\n",
    "                            dict(params=model.non_reg_params, weight_decay=0),\n",
    "                        ],\n",
    "                        lr=0.01,\n",
    "                    )\n",
    "                    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                        optimizer, mode=\"min\", factor=0.5, patience=100, verbose=False\n",
    "                    )\n",
    "\n",
    "                    # Train models\n",
    "                    best_val_acc_f1 = 0\n",
    "                    saliency, prev_out = None, None\n",
    "                    aggregator = MeanAggregation()\n",
    "\n",
    "                    all_stats_list, all_maskes_list = [], []\n",
    "                    for epoch in tqdm(\n",
    "                        range(1, args.num_epochs + 1), disable=args.disable_tqdm\n",
    "                    ):\n",
    "\n",
    "                        train()\n",
    "                        accs, baccs, f1s = test()\n",
    "                        train_acc, val_acc, tmp_test_acc = accs\n",
    "                        train_f1, val_f1, tmp_test_f1 = f1s\n",
    "                        val_acc_f1 = (val_acc + val_f1) / 2.0\n",
    "                        if val_acc_f1 > best_val_acc_f1:\n",
    "                            best_val_acc_f1 = val_acc_f1\n",
    "                            test_acc = accs[2]\n",
    "                            test_bacc = baccs[2]\n",
    "                            test_f1 = f1s[2]\n",
    "                            best_epoch = epoch\n",
    "                            y_pred = predict(model, data.x, data.edge_index)\n",
    "                            test_disparity = get_class_wise_accuracy(\n",
    "                                data.y, y_pred, data_test_mask\n",
    "                            ).std()\n",
    "\n",
    "                    avg_val_acc_f1.append(best_val_acc_f1)\n",
    "                    avg_test_acc.append(test_acc)\n",
    "                    avg_test_bacc.append(test_bacc)\n",
    "                    avg_test_f1.append(test_f1)\n",
    "                    avg_test_disparity.append(test_disparity)\n",
    "                    if args.debug:\n",
    "                        print(\n",
    "                            f\"Best epoch {best_epoch} Val Acc F1: {best_val_acc_f1:.4f}, \"\n",
    "                            f\"Test Disp {test_disparity:.4f}, Acc: {test_acc:.4f}, \"\n",
    "                            f\"BAcc: {test_bacc:.4f}, F1: {test_f1:.4f}\"\n",
    "                        )\n",
    "\n",
    "                    avg_runtime_stats = pd.DataFrame(\n",
    "                        runtime_info, columns=[\"runtime\", \"node_ratio\", \"edge_ratio\"]\n",
    "                    ).mean()\n",
    "                    avg_run_time.append(avg_runtime_stats[\"runtime\"])\n",
    "                    avg_node_ratio.append(avg_runtime_stats[\"node_ratio\"])\n",
    "                    avg_edge_ratio.append(avg_runtime_stats[\"edge_ratio\"])\n",
    "\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                if args.repeatition > 1:\n",
    "                    ## Calculate statistics ##\n",
    "                    acc_CI = statistics.stdev(avg_test_acc) / (repeatition ** (1 / 2))\n",
    "                    bacc_CI = statistics.stdev(avg_test_bacc) / (repeatition ** (1 / 2))\n",
    "                    f1_CI = statistics.stdev(avg_test_f1) / (repeatition ** (1 / 2))\n",
    "                    disp_CI = statistics.stdev(avg_test_disparity) / (\n",
    "                        repeatition ** (1 / 2)\n",
    "                    )\n",
    "                    avg_acc = statistics.mean(avg_test_acc)\n",
    "                    avg_bacc = statistics.mean(avg_test_bacc)\n",
    "                    avg_f1 = statistics.mean(avg_test_f1)\n",
    "                    avg_disp = statistics.mean(avg_test_disparity)\n",
    "                    avg_val_acc_f1 = statistics.mean(avg_val_acc_f1)\n",
    "                    avg_run_time = statistics.mean(avg_run_time)\n",
    "                    avg_node_ratio = statistics.mean(avg_node_ratio)\n",
    "                    avg_edge_ratio = statistics.mean(avg_edge_ratio)\n",
    "\n",
    "                    avg_log = \"Val Acc F1: {:.4f}, Test Disp {:.4f} +- {:.4f} Acc: {:.4f} +- {:.4f}, BAcc: {:.4f} +- {:.4f}, F1: {:.4f} +- {:.4f}\"\n",
    "                    avg_log = avg_log.format(\n",
    "                        avg_val_acc_f1,\n",
    "                        avg_disp,\n",
    "                        disp_CI,\n",
    "                        avg_acc,\n",
    "                        acc_CI,\n",
    "                        avg_bacc,\n",
    "                        bacc_CI,\n",
    "                        avg_f1,\n",
    "                        f1_CI,\n",
    "                    )\n",
    "                    avg_runtim_log = f\"node_ratio: {avg_node_ratio-1:.3%} | edge_ratio: {avg_edge_ratio-1:.3%} | Runtime: {avg_run_time:.4f} s\"\n",
    "                    log = \"{}\\n{}\\n{}\".format(setting_log, avg_runtim_log, avg_log)\n",
    "                    print(log + \"\\n\")\n",
    "\n",
    "                    run_results = [\n",
    "                        avg_val_acc_f1,\n",
    "                        avg_acc,\n",
    "                        acc_CI,\n",
    "                        avg_bacc,\n",
    "                        bacc_CI,\n",
    "                        avg_f1,\n",
    "                        f1_CI,\n",
    "                        avg_disp,\n",
    "                        disp_CI,\n",
    "                        model_name,\n",
    "                        dataset_name,\n",
    "                        baseline_name,\n",
    "                        gba_setting,\n",
    "                        avg_run_time,\n",
    "                        avg_node_ratio,\n",
    "                        avg_edge_ratio,\n",
    "                    ]\n",
    "                    all_results.append(run_results)\n",
    "\n",
    "                    if args.save_results:\n",
    "                        columns = [\n",
    "                            \"avg_val_acc_f1\",\n",
    "                            \"acc\",\n",
    "                            \"acc_std\",\n",
    "                            \"bacc\",\n",
    "                            \"bacc_std\",\n",
    "                            \"f1\",\n",
    "                            \"f1_std\",\n",
    "                            \"disparity\",\n",
    "                            \"disparity_std\",\n",
    "                            \"model\",\n",
    "                            \"dataset\",\n",
    "                            \"baseline\",\n",
    "                            \"setting\",\n",
    "                            \"runtime\",\n",
    "                            \"node_ratio\",\n",
    "                            \"edge_ratio\",\n",
    "                        ]\n",
    "                        df_all_results = pd.DataFrame(all_results, columns=columns)\n",
    "                        df_all_results.to_csv(args.res_path, index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
